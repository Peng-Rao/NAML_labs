{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_svd(A, k):\n",
    "    \"\"\"\n",
    "    Compute the truncated SVD of a matrix A using NumPy.\n",
    "\n",
    "    Parameters:\n",
    "        A (ndarray): Input matrix of shape (m, n).\n",
    "        k (int): Number of singular values and vectors to retain.\n",
    "\n",
    "    Returns:\n",
    "        U_k (ndarray): Truncated left singular vectors, shape (m, k).\n",
    "        S_k (ndarray): Truncated singular values, shape (k,).\n",
    "        Vt_k (ndarray): Truncated right singular vectors, shape (k, n).\n",
    "    \"\"\"\n",
    "    # Perform full SVD\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "    # Truncate to the first k singular values/vectors\n",
    "    U_k = U[:, :k]\n",
    "    S_k = S[:k]\n",
    "    Vt_k = Vt[:k, :]\n",
    "\n",
    "    return U_k, S_k, Vt_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVR:\n",
    "    def __init__(self, epsilon=0.1, lmbda=1.0):\n",
    "        self.epsilon = epsilon\n",
    "        self.lmbda = lmbda\n",
    "        self.w = None\n",
    "\n",
    "    def loss(self, params, X, y):\n",
    "        predictions = jnp.dot(X, params[:-1]) + params[-1]\n",
    "\n",
    "        # Compute epsilon-insensitive loss\n",
    "        epsilon_loss = jnp.maximum(0, jnp.abs(predictions - y) - self.epsilon)\n",
    "\n",
    "        # Regularization term (L2 norm of w)\n",
    "        reg_term = self.lmbda * jnp.sum(params**2)\n",
    "\n",
    "        # Total loss\n",
    "        return reg_term + jnp.mean(epsilon_loss)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = jnp.zeros(n_features + 1)\n",
    "\n",
    "        # Solve optimization problem\n",
    "        opt_res = jax.scipy.optimize.minimize(\n",
    "            self.loss, self.w, method=\"BFGS\", args=(X, y)\n",
    "        )\n",
    "        self.w = opt_res.x\n",
    "\n",
    "    def predict(self, X):\n",
    "        return jnp.dot(X, self.w[:-1]) + self.w[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, lmbda=1.0):\n",
    "        self.lmbda = lmbda\n",
    "        self.w = None\n",
    "\n",
    "    def loss(self, params, X, y):\n",
    "        # Compute the decision function\n",
    "        decision = jnp.dot(X, params[:-1]) + params[-1]\n",
    "        # Compute the hinge loss\n",
    "        loss_val = jnp.maximum(0, 1 - y * decision)\n",
    "        # Regularization term (L2 norm of w)\n",
    "        reg_term = self.lmbda * jnp.sum(params**2)\n",
    "        # Total loss\n",
    "        return reg_term + jnp.mean(loss_val)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = jnp.zeros(n_features + 1)\n",
    "\n",
    "        # Solve optimization problem\n",
    "        opt_res = jax.scipy.optimize.minimize(\n",
    "            self.loss, self.w, method=\"BFGS\", args=(X, y)\n",
    "        )\n",
    "        self.w = opt_res.x\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Decision function\n",
    "        decision = jnp.dot(X, self.w[:-1]) + self.w[-1]\n",
    "        return jnp.sign(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(func, dfunc, x_0, stepsize, tol, max_iter):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to minimize a given function.\n",
    "\n",
    "    Parameters:\n",
    "    func (callable): The function to minimize\n",
    "    dfunc (callable): The gradient of the function\n",
    "    x_0 (np.array): Initial guess for the minimizer\n",
    "    stepsize (float): Constant step size for gradient descent\n",
    "    tol (float): Tolerance for stopping criterion\n",
    "    max_iter (int): Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    np.array: The point minimizing the function\n",
    "    float: The minimum value of the function\n",
    "    list: List of points visited during gradient descent\n",
    "    \"\"\"\n",
    "    x = np.array(x_0, dtype=float)  # Initialize current point\n",
    "    path = [x.copy()]  # Store the path of points visited\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        grad = dfunc(x)  # Compute gradient\n",
    "        x_new = x - stepsize * grad  # Update step using gradient descent\n",
    "\n",
    "        # Save the new point in the path\n",
    "        path.append(x_new.copy())\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            print(f\"Converged in {iteration + 1} iterations.\")\n",
    "            return x_new, func(x_new), path\n",
    "\n",
    "        x = x_new  # Update current point\n",
    "\n",
    "    print(\"Reached the maximum number of iterations.\")\n",
    "    return x, func(x), path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2*x + 3*y, 3*x + 2*y)\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "x, y = symbols(\"x y\")\n",
    "f = x**2 + 3 * x * y + y**2\n",
    "grad_f = (diff(f, x), diff(f, y))\n",
    "print(grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Hessian Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian Matrix:\n",
      "Matrix([[1200*x**2 - 400*y + 2, -400*x], [-400*x, 200]])\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff, Matrix\n",
    "\n",
    "# Define the variables\n",
    "x, y = symbols(\"x y\")\n",
    "\n",
    "# Define the function\n",
    "f = 100 * (y - x**2) ** 2 + (1 - x) ** 2\n",
    "\n",
    "# Compute the first-order partial derivatives\n",
    "f_x = diff(f, x)\n",
    "f_y = diff(f, y)\n",
    "\n",
    "# Compute the second-order partial derivatives\n",
    "f_xx = diff(f_x, x)\n",
    "f_xy = diff(f_x, y)\n",
    "f_yy = diff(f_y, y)\n",
    "\n",
    "# Create the Hessian matrix\n",
    "hessian = Matrix([[f_xx, f_xy], [f_xy, f_yy]])\n",
    "\n",
    "# Display the Hessian matrix\n",
    "print(\"Hessian Matrix:\")\n",
    "print(hessian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [-400*x*(-x**2 + y) + 2*x - 2, -200*x**2 + 200*y]\n",
      "Hessian:\n",
      " Matrix([[1200*x**2 - 400*y + 2, -400*x], [-400*x, 200]])\n",
      "Critical Points: [(1, 1)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff, hessian, solve\n",
    "\n",
    "# Define symbols\n",
    "x, y = symbols(\"x y\")\n",
    "\n",
    "# Define the function\n",
    "f = 100 * (y - x**2) ** 2 + (1 - x) ** 2\n",
    "\n",
    "# Compute the gradient\n",
    "grad_f = [diff(f, var) for var in (x, y)]\n",
    "\n",
    "# Compute the Hessian\n",
    "hessian_f = hessian(f, (x, y))\n",
    "\n",
    "# Solve for critical points (gradient = 0)\n",
    "critical_points = solve(grad_f, (x, y))\n",
    "\n",
    "# Print results\n",
    "print(\"Gradient:\", grad_f)\n",
    "print(\"Hessian:\\n\", hessian_f)\n",
    "print(\"Critical Points:\", critical_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
